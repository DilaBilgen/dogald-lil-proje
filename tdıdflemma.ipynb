{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ed32d66-e540-4468-9396-ff0295b05b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatize edilmiş cümleler için TF-IDF hesaplandı ve dosyaya kaydedildi.\n",
      "\n",
      "--- TF-IDF Matrisi İlk 5 Satır ---\n",
      "   aarp  access  account  ace  action   ad  adapted  add  added  adventure  \\\n",
      "0   0.0     0.0      0.0  0.0     0.0  0.0      0.0  0.0    0.0        0.0   \n",
      "1   0.0     0.0      0.0  0.0     0.0  0.0      0.0  0.0    0.0        0.0   \n",
      "2   0.0     0.0      0.0  0.0     0.0  0.0      0.0  0.0    0.0        0.0   \n",
      "3   0.0     0.0      0.0  0.0     0.0  0.0      0.0  0.0    0.0        0.0   \n",
      "4   0.0     0.0      0.0  0.0     0.0  0.0      0.0  0.0    0.0        0.0   \n",
      "\n",
      "   ...  world  www  xtream  year  yeti  yield  york  young  youtube  ziel  \n",
      "0  ...    0.0  0.0     0.0   0.0   0.0    0.0   0.0    0.0      0.0   0.0  \n",
      "1  ...    0.0  0.0     0.0   0.0   0.0    0.0   0.0    0.0      0.0   0.0  \n",
      "2  ...    0.0  0.0     0.0   0.0   0.0    0.0   0.0    0.0      0.0   0.0  \n",
      "3  ...    0.0  0.0     0.0   0.0   0.0    0.0   0.0    0.0      0.0   0.0  \n",
      "4  ...    0.0  0.0     0.0   0.0   0.0    0.0   0.0    0.0      0.0   0.0  \n",
      "\n",
      "[5 rows x 1000 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "dosya_yolu = pd.read_csv(\"lemmatize_edilmis_yorumlar.csv\")\n",
    "\n",
    "def temizle_ve_duzleştir(x):\n",
    "    try:\n",
    "        liste = eval(str(x))\n",
    "        if all(isinstance(i, list) for i in liste):\n",
    "            return [item for alt in liste for item in alt]\n",
    "        return liste\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "token_liste = dosya_yolu[\"lemmatize_cümleler\"].apply(temizle_ve_duzleştir)\n",
    "\n",
    "lemmatized_texts = [' '.join(tokens) for tokens in token_liste]\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=1000)\n",
    "tfidf_matrix = tfidf.fit_transform(lemmatized_texts)\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())\n",
    "\n",
    "tfidf_df.to_csv(\"tfidf_lemmatized_yorumlar.csv\", index=False)\n",
    "print(\"Lemmatize edilmiş cümleler için TF-IDF hesaplandı ve dosyaya kaydedildi.\")\n",
    "print(\"\\n--- TF-IDF Matrisi İlk 5 Satır ---\")\n",
    "print(tfidf_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5da933-024e-4ade-b457-f6db56dc4215",
   "metadata": {},
   "outputs": [],
   "source": [
    "sS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
